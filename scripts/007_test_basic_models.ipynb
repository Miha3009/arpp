{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40ea4497-4bd2-42ce-92f2-48f74fa04c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import ClimateModel, BaseModel, LinearRegression, BoostingModel, MLP\n",
    "from train import make_train_test, evaluate_print\n",
    "\n",
    "FIRST_YEAR = 1991\n",
    "SPLIT_YEAR = 2017\n",
    "LAST_YEAR = 2019\n",
    "\n",
    "def make_ds(element, args):\n",
    "    return make_train_test(element, FIRST_YEAR, SPLIT_YEAR, LAST_YEAR, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a43119d-2996-4cef-b3f7-f919e4a56e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля морского льда, март\n",
      "                                  Климат, train   0.1053, test   0.1190\n",
      "                     Несмещенный прогноз, train   0.0690, test   0.0627\n",
      "                      Линейная регрессия, train   0.0633, test   0.0612\n",
      "                                LightGBM, train   0.0505, test   0.0570\n",
      "                  Полносвязная нейросеть, train   0.0567, test   0.0574\n",
      "Доля морского льда, август\n",
      "                                  Климат, train   0.2420, test   0.2369\n",
      "                     Несмещенный прогноз, train   0.0658, test   0.0884\n",
      "                      Линейная регрессия, train   0.0539, test   0.0651\n",
      "                                LightGBM, train   0.0366, test   0.0526\n",
      "                  Полносвязная нейросеть, train   0.0448, test   0.0545\n"
     ]
    }
   ],
   "source": [
    "dsTrain, dsTest = make_ds('aice', args={\"lead_times\": [3], \"periods\": [3], \"normed\": True, \"use_cache\": True})\n",
    "variables = dsTrain.variables.copy()\n",
    "models = [ClimateModel(), BaseModel(), LinearRegression(variables=variables), BoostingModel(variables=variables),\n",
    "         MLP(variant='aice', input_size=len(dsTrain.variables), hidden_layers=[16, 8], test=dsTest, lr=0.003, epochs=170)]\n",
    "\n",
    "print('Доля морского льда, март')\n",
    "for model in models:\n",
    "    model.fit(dsTrain)\n",
    "    evaluate_print(model, dsTrain, dsTest)\n",
    "\n",
    "print('Доля морского льда, август')\n",
    "dsTrain, dsTest = make_ds('aice', args={\"lead_times\": [3], \"periods\": [8]})\n",
    "for model in models:\n",
    "    model.fit(dsTrain)\n",
    "    evaluate_print(model, dsTrain, dsTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef393741-d57e-4c5f-8ada-0660ee1568a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Водный эквивалент снега, ноябрь\n",
      "                                  Климат, train  12.4006, test  11.1988\n",
      "                     Несмещенный прогноз, train  12.7006, test  11.5499\n",
      "                      Линейная регрессия, train  12.2444, test  11.2351\n",
      "                                LightGBM, train  11.8255, test  11.1493\n",
      "                  Полносвязная нейросеть, train  12.2983, test  11.0934\n"
     ]
    }
   ],
   "source": [
    "dsTrain, dsTest = make_ds('swe', args={\"lead_times\": [3], \"periods\": list(range(44, 49)), \"normed\": True, \"use_cache\": True})\n",
    "variables = dsTrain.variables.copy()\n",
    "models = [ClimateModel(), BaseModel(), LinearRegression(variables=variables), BoostingModel(variables=variables),\n",
    "         MLP(variant='swe', input_size=len(dsTrain.variables), hidden_layers=[16, 8], test=dsTest, lr=0.01, epochs=17)]\n",
    "\n",
    "print('Водный эквивалент снега, ноябрь')\n",
    "for model in models:\n",
    "     model.fit(dsTrain)\n",
    "     evaluate_print(model, dsTrain, dsTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c120c55-f62e-4a6b-973c-28644a8abb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1, train loss:   0.1505, test loss:   0.1650\n",
      "Epoch   2, train loss:   0.1465, test loss:   0.1611\n",
      "Epoch   3, train loss:   0.1427, test loss:   0.1574\n",
      "Epoch   4, train loss:   0.1391, test loss:   0.1539\n",
      "Epoch   5, train loss:   0.1359, test loss:   0.1508\n",
      "Epoch   6, train loss:   0.1330, test loss:   0.1480\n",
      "Epoch   7, train loss:   0.1305, test loss:   0.1456\n",
      "Epoch   8, train loss:   0.1284, test loss:   0.1434\n",
      "Epoch   9, train loss:   0.1265, test loss:   0.1414\n",
      "Epoch  10, train loss:   0.1248, test loss:   0.1394\n",
      "Epoch  11, train loss:   0.1231, test loss:   0.1375\n",
      "Epoch  12, train loss:   0.1214, test loss:   0.1356\n",
      "Epoch  13, train loss:   0.1197, test loss:   0.1336\n",
      "Epoch  14, train loss:   0.1180, test loss:   0.1316\n",
      "Epoch  15, train loss:   0.1163, test loss:   0.1296\n",
      "Epoch  16, train loss:   0.1145, test loss:   0.1274\n",
      "Epoch  17, train loss:   0.1126, test loss:   0.1252\n",
      "Epoch  18, train loss:   0.1106, test loss:   0.1228\n",
      "Epoch  19, train loss:   0.1085, test loss:   0.1203\n",
      "Epoch  20, train loss:   0.1064, test loss:   0.1176\n",
      "Epoch  21, train loss:   0.1042, test loss:   0.1147\n",
      "Epoch  22, train loss:   0.1020, test loss:   0.1118\n",
      "Epoch  23, train loss:   0.0998, test loss:   0.1086\n",
      "Epoch  24, train loss:   0.0976, test loss:   0.1055\n",
      "Epoch  25, train loss:   0.0955, test loss:   0.1022\n",
      "Epoch  26, train loss:   0.0935, test loss:   0.0990\n",
      "Epoch  27, train loss:   0.0916, test loss:   0.0958\n",
      "Epoch  28, train loss:   0.0899, test loss:   0.0928\n",
      "Epoch  29, train loss:   0.0882, test loss:   0.0900\n",
      "Epoch  30, train loss:   0.0868, test loss:   0.0875\n",
      "Epoch  31, train loss:   0.0854, test loss:   0.0854\n",
      "Epoch  32, train loss:   0.0842, test loss:   0.0838\n",
      "Epoch  33, train loss:   0.0831, test loss:   0.0826\n",
      "Epoch  34, train loss:   0.0822, test loss:   0.0820\n",
      "Epoch  35, train loss:   0.0813, test loss:   0.0819\n",
      "Epoch  36, train loss:   0.0805, test loss:   0.0821\n",
      "Epoch  37, train loss:   0.0798, test loss:   0.0824\n",
      "Epoch  38, train loss:   0.0790, test loss:   0.0828\n",
      "Epoch  39, train loss:   0.0781, test loss:   0.0831\n",
      "Epoch  40, train loss:   0.0773, test loss:   0.0832\n",
      "Epoch  41, train loss:   0.0764, test loss:   0.0831\n",
      "Epoch  42, train loss:   0.0754, test loss:   0.0827\n",
      "Epoch  43, train loss:   0.0745, test loss:   0.0821\n",
      "Epoch  44, train loss:   0.0735, test loss:   0.0813\n",
      "Epoch  45, train loss:   0.0727, test loss:   0.0804\n",
      "Epoch  46, train loss:   0.0718, test loss:   0.0794\n",
      "Epoch  47, train loss:   0.0710, test loss:   0.0783\n",
      "Epoch  48, train loss:   0.0703, test loss:   0.0771\n",
      "Epoch  49, train loss:   0.0696, test loss:   0.0759\n",
      "Epoch  50, train loss:   0.0689, test loss:   0.0747\n",
      "Epoch  51, train loss:   0.0682, test loss:   0.0735\n",
      "Epoch  52, train loss:   0.0675, test loss:   0.0722\n",
      "Epoch  53, train loss:   0.0668, test loss:   0.0708\n",
      "Epoch  54, train loss:   0.0661, test loss:   0.0695\n",
      "Epoch  55, train loss:   0.0654, test loss:   0.0681\n",
      "Epoch  56, train loss:   0.0647, test loss:   0.0668\n",
      "Epoch  57, train loss:   0.0641, test loss:   0.0655\n",
      "Epoch  58, train loss:   0.0636, test loss:   0.0644\n",
      "Epoch  59, train loss:   0.0633, test loss:   0.0634\n",
      "Epoch  60, train loss:   0.0630, test loss:   0.0625\n",
      "Epoch  61, train loss:   0.0628, test loss:   0.0618\n",
      "Epoch  62, train loss:   0.0627, test loss:   0.0612\n",
      "Epoch  63, train loss:   0.0627, test loss:   0.0608\n",
      "Epoch  64, train loss:   0.0626, test loss:   0.0605\n",
      "Epoch  65, train loss:   0.0625, test loss:   0.0603\n",
      "Epoch  66, train loss:   0.0624, test loss:   0.0601\n",
      "Epoch  67, train loss:   0.0623, test loss:   0.0600\n",
      "Epoch  68, train loss:   0.0622, test loss:   0.0599\n",
      "Epoch  69, train loss:   0.0621, test loss:   0.0599\n",
      "Epoch  70, train loss:   0.0620, test loss:   0.0598\n",
      "Epoch  71, train loss:   0.0619, test loss:   0.0598\n",
      "Epoch  72, train loss:   0.0618, test loss:   0.0597\n",
      "Epoch  73, train loss:   0.0617, test loss:   0.0597\n",
      "Epoch  74, train loss:   0.0616, test loss:   0.0596\n",
      "Epoch  75, train loss:   0.0615, test loss:   0.0595\n",
      "Epoch  76, train loss:   0.0614, test loss:   0.0595\n",
      "Epoch  77, train loss:   0.0613, test loss:   0.0595\n",
      "Epoch  78, train loss:   0.0613, test loss:   0.0595\n",
      "Epoch  79, train loss:   0.0612, test loss:   0.0594\n",
      "Epoch  80, train loss:   0.0611, test loss:   0.0595\n",
      "Epoch  81, train loss:   0.0610, test loss:   0.0595\n",
      "Epoch  82, train loss:   0.0609, test loss:   0.0595\n",
      "Epoch  83, train loss:   0.0609, test loss:   0.0595\n",
      "Epoch  84, train loss:   0.0608, test loss:   0.0595\n",
      "Epoch  85, train loss:   0.0608, test loss:   0.0596\n",
      "Epoch  86, train loss:   0.0607, test loss:   0.0596\n",
      "Epoch  87, train loss:   0.0607, test loss:   0.0596\n",
      "Epoch  88, train loss:   0.0606, test loss:   0.0597\n",
      "Epoch  89, train loss:   0.0606, test loss:   0.0597\n",
      "Epoch  90, train loss:   0.0606, test loss:   0.0597\n",
      "Epoch  91, train loss:   0.0605, test loss:   0.0597\n",
      "Epoch  92, train loss:   0.0605, test loss:   0.0597\n",
      "Epoch  93, train loss:   0.0605, test loss:   0.0597\n",
      "Epoch  94, train loss:   0.0604, test loss:   0.0596\n",
      "Epoch  95, train loss:   0.0604, test loss:   0.0596\n",
      "Epoch  96, train loss:   0.0603, test loss:   0.0595\n",
      "Epoch  97, train loss:   0.0603, test loss:   0.0595\n",
      "Epoch  98, train loss:   0.0603, test loss:   0.0594\n",
      "Epoch  99, train loss:   0.0602, test loss:   0.0594\n",
      "Epoch 100, train loss:   0.0602, test loss:   0.0593\n",
      "Epoch 101, train loss:   0.0601, test loss:   0.0593\n",
      "Epoch 102, train loss:   0.0601, test loss:   0.0592\n",
      "Epoch 103, train loss:   0.0601, test loss:   0.0592\n",
      "Epoch 104, train loss:   0.0600, test loss:   0.0591\n",
      "Epoch 105, train loss:   0.0600, test loss:   0.0590\n",
      "Epoch 106, train loss:   0.0600, test loss:   0.0590\n",
      "Epoch 107, train loss:   0.0599, test loss:   0.0590\n",
      "Epoch 108, train loss:   0.0599, test loss:   0.0589\n",
      "Epoch 109, train loss:   0.0599, test loss:   0.0589\n",
      "Epoch 110, train loss:   0.0598, test loss:   0.0588\n",
      "Epoch 111, train loss:   0.0598, test loss:   0.0588\n",
      "Epoch 112, train loss:   0.0598, test loss:   0.0588\n",
      "Epoch 113, train loss:   0.0597, test loss:   0.0587\n",
      "Epoch 114, train loss:   0.0597, test loss:   0.0587\n",
      "Epoch 115, train loss:   0.0596, test loss:   0.0587\n",
      "Epoch 116, train loss:   0.0596, test loss:   0.0587\n",
      "Epoch 117, train loss:   0.0596, test loss:   0.0586\n",
      "Epoch 118, train loss:   0.0595, test loss:   0.0586\n",
      "Epoch 119, train loss:   0.0595, test loss:   0.0586\n",
      "Epoch 120, train loss:   0.0595, test loss:   0.0586\n",
      "Epoch 121, train loss:   0.0594, test loss:   0.0586\n",
      "Epoch 122, train loss:   0.0594, test loss:   0.0586\n",
      "Epoch 123, train loss:   0.0594, test loss:   0.0586\n",
      "Epoch 124, train loss:   0.0593, test loss:   0.0586\n",
      "Epoch 125, train loss:   0.0593, test loss:   0.0586\n",
      "Epoch 126, train loss:   0.0593, test loss:   0.0586\n",
      "Epoch 127, train loss:   0.0592, test loss:   0.0586\n",
      "Epoch 128, train loss:   0.0592, test loss:   0.0586\n",
      "Epoch 129, train loss:   0.0592, test loss:   0.0586\n",
      "Epoch 130, train loss:   0.0591, test loss:   0.0586\n",
      "Epoch 131, train loss:   0.0591, test loss:   0.0586\n",
      "Epoch 132, train loss:   0.0591, test loss:   0.0586\n",
      "Epoch 133, train loss:   0.0590, test loss:   0.0586\n",
      "Epoch 134, train loss:   0.0590, test loss:   0.0585\n",
      "Epoch 135, train loss:   0.0589, test loss:   0.0585\n",
      "Epoch 136, train loss:   0.0589, test loss:   0.0585\n",
      "Epoch 137, train loss:   0.0589, test loss:   0.0585\n",
      "Epoch 138, train loss:   0.0588, test loss:   0.0585\n",
      "Epoch 139, train loss:   0.0588, test loss:   0.0584\n",
      "Epoch 140, train loss:   0.0587, test loss:   0.0584\n",
      "Epoch 141, train loss:   0.0587, test loss:   0.0584\n",
      "Epoch 142, train loss:   0.0587, test loss:   0.0583\n",
      "Epoch 143, train loss:   0.0586, test loss:   0.0583\n",
      "Epoch 144, train loss:   0.0586, test loss:   0.0583\n",
      "Epoch 145, train loss:   0.0585, test loss:   0.0582\n",
      "Epoch 146, train loss:   0.0585, test loss:   0.0582\n",
      "Epoch 147, train loss:   0.0584, test loss:   0.0582\n",
      "Epoch 148, train loss:   0.0584, test loss:   0.0581\n",
      "Epoch 149, train loss:   0.0583, test loss:   0.0581\n",
      "Epoch 150, train loss:   0.0583, test loss:   0.0580\n",
      "Epoch 151, train loss:   0.0582, test loss:   0.0580\n",
      "Epoch 152, train loss:   0.0582, test loss:   0.0580\n",
      "Epoch 153, train loss:   0.0581, test loss:   0.0579\n",
      "Epoch 154, train loss:   0.0580, test loss:   0.0579\n",
      "Epoch 155, train loss:   0.0580, test loss:   0.0579\n",
      "Epoch 156, train loss:   0.0579, test loss:   0.0578\n",
      "Epoch 157, train loss:   0.0578, test loss:   0.0578\n",
      "Epoch 158, train loss:   0.0578, test loss:   0.0578\n",
      "Epoch 159, train loss:   0.0577, test loss:   0.0578\n",
      "Epoch 160, train loss:   0.0576, test loss:   0.0577\n",
      "Epoch 161, train loss:   0.0575, test loss:   0.0577\n",
      "Epoch 162, train loss:   0.0575, test loss:   0.0577\n",
      "Epoch 163, train loss:   0.0574, test loss:   0.0577\n",
      "Epoch 164, train loss:   0.0573, test loss:   0.0577\n",
      "Epoch 165, train loss:   0.0572, test loss:   0.0576\n",
      "Epoch 166, train loss:   0.0571, test loss:   0.0576\n",
      "Epoch 167, train loss:   0.0570, test loss:   0.0575\n",
      "Epoch 168, train loss:   0.0569, test loss:   0.0575\n",
      "Epoch 169, train loss:   0.0569, test loss:   0.0574\n",
      "Epoch 170, train loss:   0.0568, test loss:   0.0574\n"
     ]
    }
   ],
   "source": [
    "dsTrain, dsTest = make_ds('aice', args={\"lead_times\": [3], \"periods\": [3], \"normed\": True, \"use_cache\": True})\n",
    "model = MLP(variant='aice', input_size=len(dsTrain.variables), test=dsTest, lr=0.003, epochs=170, verbose=True)\n",
    "_ = model.fit(dsTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cfc3d84-4292-4384-952f-7f2390e0c6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1, train loss:  12.4041, test loss:  11.1814\n",
      "Epoch   2, train loss:  12.4021, test loss:  11.1812\n",
      "Epoch   3, train loss:  12.4004, test loss:  11.1800\n",
      "Epoch   4, train loss:  12.3985, test loss:  11.1780\n",
      "Epoch   5, train loss:  12.3961, test loss:  11.1751\n",
      "Epoch   6, train loss:  12.3932, test loss:  11.1713\n",
      "Epoch   7, train loss:  12.3895, test loss:  11.1664\n",
      "Epoch   8, train loss:  12.3852, test loss:  11.1602\n",
      "Epoch   9, train loss:  12.3799, test loss:  11.1529\n",
      "Epoch  10, train loss:  12.3738, test loss:  11.1445\n",
      "Epoch  11, train loss:  12.3668, test loss:  11.1350\n",
      "Epoch  12, train loss:  12.3588, test loss:  11.1248\n",
      "Epoch  13, train loss:  12.3499, test loss:  11.1144\n",
      "Epoch  14, train loss:  12.3401, test loss:  11.1043\n",
      "Epoch  15, train loss:  12.3296, test loss:  11.0957\n",
      "Epoch  16, train loss:  12.3185, test loss:  11.0894\n",
      "Epoch  17, train loss:  12.3072, test loss:  11.0861\n",
      "Epoch  18, train loss:  12.2960, test loss:  11.0867\n",
      "Epoch  19, train loss:  12.2854, test loss:  11.0921\n",
      "Epoch  20, train loss:  12.2759, test loss:  11.1026\n",
      "Epoch  21, train loss:  12.2685, test loss:  11.1177\n",
      "Epoch  22, train loss:  12.2637, test loss:  11.1354\n",
      "Epoch  23, train loss:  12.2618, test loss:  11.1523\n",
      "Epoch  24, train loss:  12.2621, test loss:  11.1652\n",
      "Epoch  25, train loss:  12.2632, test loss:  11.1720\n",
      "Epoch  26, train loss:  12.2633, test loss:  11.1717\n",
      "Epoch  27, train loss:  12.2613, test loss:  11.1650\n",
      "Epoch  28, train loss:  12.2573, test loss:  11.1540\n",
      "Epoch  29, train loss:  12.2520, test loss:  11.1416\n",
      "Epoch  30, train loss:  12.2464, test loss:  11.1299\n"
     ]
    }
   ],
   "source": [
    "dsTrain, dsTest = make_ds('swe', args={\"lead_times\": [3], \"periods\": list(range(44, 49)), \"normed\": True, \"use_cache\": True})\n",
    "model = MLP(variant='swe', input_size=len(dsTrain.variables), test=dsTest, lr=0.01, epochs=30, verbose=True)\n",
    "_ = model.fit(dsTrain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
